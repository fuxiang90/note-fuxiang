


## 第五章
这一章主要是讲了 一些优化的问题
首先全部都要根据问题，构造一个花费函数 cost 。

有很多np 问题，都是采用这章所介绍的随机算法解决的。
### 随机算法 
随机构造一些解，取里面的最优解作为最终解。


### 爬山算法
cost = f(x) ,如果 x 向左 or 向右 可以使解 优化，那么x 取新的点，知道得到一个局部最优解。 

### 模拟退火
> 爬山法是完完全全的贪心法，每次都鼠目寸光的选择一个当前最优解，因此只能搜索到局部的最优值。模拟退火其实> 也是一种贪心算法，但是它的搜索过程引入了随机因素。模拟退火算法以一定的概率来接受一个比当前解要差的解， 
> 因此有可能会跳出这个局部的最优解，达到全局的最优解。以图1为例，模拟退火算法在搜索到局部最优解A后，会以
> 一定的概率接受到E的移动。也许经过几次这样的不是局部最优的移动后会到达D点，于是就跳出了局部最大值A。

模拟退火算法是一种随机算法，并不一定能找到全局的最优解，可以比较快的找到问题的近似最优解。 如果参数设置得当，模拟退火算法搜索效率比穷举法要高。
### 遗传算法
>　　借鉴生物进化论，遗传算法将要解决的问题模拟成一个生物进化的过程，通过复制、交叉、突变等操作产生下一代的解，并逐步淘汰掉适应度函数值低的解，增加适应度函数值高的解。这样进化N代后就很有可能会进化出适应度函数值很高的个体。


###总结
>1. 数据预处理 
       在训练神经网络前一般需要对数据进行预处理，一种重要的预处理手段是归一化处理。下面简要介绍归一化处理的原理与方法。

(1) 什么是归一化？ 

数据归一化，就是将数据映射到[0,1]或[-1,1]区间或更小的区间，比如(0.1,0.9) 。

(2) 为什么要归一化处理？ 

<1>输入数据的单位不一样，有些数据的范围可能特别大，导致的结果是神经网络收敛慢、训练时间长。

<2>数据范围大的输入在模式分类中的作用可能会偏大，而数据范围小的输入作用就可能会偏小。

<3>由于神经网络输出层的激活函数的值域是有限制的，因此需要将网络训练的目标数据映射到激活函数的值域。例如神经网络的输出层若采用S形激活函数，由于S形函数的值域限制在(0,1)，也就是说神经网络的输出只能限制在(0,1)，所以训练数据的输出就要归一化到[0,1]区间。

<4>S形激活函数在(0,1)区间以外区域很平缓，区分度太小。例如S形函数f(X)在参数a=1时，f(100)与f(5)只相差0.0067。


这个问题 我在之前写道路匹配遇到过，看来下次最好弄个sigmod 的归一化


### 参考
[1](http://www.cnblogs.com/heaad/archive/2010/12/20/1911614.html)    
[2](http://blog.csdn.net/v_JULY_v/article/details/6132775)     
[3](http://www.cnblogs.com/heaad/archive/2010/12/23/1914725.html)     